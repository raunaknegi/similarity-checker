{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T14:31:18.462005Z",
     "iopub.status.busy": "2021-06-08T14:31:18.461539Z",
     "iopub.status.idle": "2021-06-08T14:31:26.863174Z",
     "shell.execute_reply": "2021-06-08T14:31:26.862367Z",
     "shell.execute_reply.started": "2021-06-08T14:31:18.461955Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "import tensorflow_hub as hub\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T14:31:06.550312Z",
     "iopub.status.busy": "2021-06-08T14:31:06.549964Z",
     "iopub.status.idle": "2021-06-08T14:31:18.459309Z",
     "shell.execute_reply": "2021-06-08T14:31:18.457981Z",
     "shell.execute_reply.started": "2021-06-08T14:31:06.550283Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers\n",
    "# install this transformer to download BERT pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T12:44:46.549082Z",
     "iopub.status.busy": "2021-06-08T12:44:46.548702Z",
     "iopub.status.idle": "2021-06-08T12:44:46.554565Z",
     "shell.execute_reply": "2021-06-08T12:44:46.553496Z",
     "shell.execute_reply.started": "2021-06-08T12:44:46.549052Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf_idf\n",
    "def tf_idf(document1, document2):\n",
    "    vectorizer=TfidfVectorizer()\n",
    "    document_appended=np.array((document1,document2))\n",
    "    embeddings=vectorizer.fit_transform(document_appended)\n",
    "    score=cosine_similarity(embeddings[0:1],embeddings[1:])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T14:49:22.549767Z",
     "iopub.status.busy": "2021-06-08T14:49:22.548705Z",
     "iopub.status.idle": "2021-06-08T14:49:22.560016Z",
     "shell.execute_reply": "2021-06-08T14:49:22.558738Z",
     "shell.execute_reply.started": "2021-06-08T14:49:22.549702Z"
    }
   },
   "outputs": [],
   "source": [
    "#using BERT,here every sentence is compared against others\n",
    "def BERT(document1,document2):\n",
    "#     use this line to download the pretrained model\n",
    "#     model=SentenceTransformer('bert-base-nli-mean-tokens') \n",
    "\n",
    "    doc1_tokenized=re.compile('[.!?]').split(document1)[:-1]\n",
    "    doc2_tokenized=re.compile('[.!?]').split(document2)[:-1]\n",
    "\n",
    "    embedding1=np.array(model.encode(doc1_tokenized))\n",
    "    embedding2=np.array(model.encode(doc2_tokenized))\n",
    "\n",
    "    score=cosine_similarity(embedding1,embedding2)\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T14:53:36.071526Z",
     "iopub.status.busy": "2021-06-08T14:53:36.071140Z",
     "iopub.status.idle": "2021-06-08T14:53:36.076606Z",
     "shell.execute_reply": "2021-06-08T14:53:36.075501Z",
     "shell.execute_reply.started": "2021-06-08T14:53:36.071494Z"
    }
   },
   "outputs": [],
   "source": [
    "#using Universal Sentence Encoder\n",
    "def USE(document1,document2):\n",
    "    #use the module url to get the Encoder model\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "    \n",
    "    #uncomment this line while running on your system\n",
    "#     model=hub.load(module_url)\n",
    "\n",
    "    vec1=model([document1])\n",
    "    vec2=model([document2])\n",
    "\n",
    "    score=cosine_similarity(vec1,vec2)\n",
    "    return score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T12:45:03.186750Z",
     "iopub.status.busy": "2021-06-08T12:45:03.186171Z",
     "iopub.status.idle": "2021-06-08T12:45:03.195233Z",
     "shell.execute_reply": "2021-06-08T12:45:03.194259Z",
     "shell.execute_reply.started": "2021-06-08T12:45:03.186700Z"
    }
   },
   "outputs": [],
   "source": [
    "#jaccard similarity\n",
    "def jaccard_similarity(document1,document2):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    def preprocess(text):\n",
    "        text=re.sub('[^a-zA-Z]',' ',text)\n",
    "        text=text.lower().split()\n",
    "        text=[lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "        text=' '.join(text)\n",
    "        return text\n",
    "\n",
    "    doc1_tokenized=preprocess(document1)\n",
    "    doc2_tokenized=preprocess(document2)\n",
    "\n",
    "    union=set(doc1_tokenized+doc2_tokenized)\n",
    "\n",
    "    intersection=set()\n",
    "\n",
    "    for word in doc1_tokenized:\n",
    "        if word in doc2_tokenized:\n",
    "            intersection.add(word)\n",
    "\n",
    "    score=len(intersection)/len(union)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
